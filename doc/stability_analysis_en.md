# Analysis of Personality Stability (Convergence)

This document outlines the design and implementation for analyzing the stability of personality traits generated by a Large Language Model (LLM) for a single persona.

## 1. Objective

The primary goal is to determine if an LLM, when prompted with the same persona description repeatedly, produces a consistent and stable personality profile. If the results are stable, it suggests the LLM has formed a coherent internal representation of the persona. We refer to this as stability or convergence.

The analysis is based on a dataset where a single persona undergoes a personality test (e.g., IPIP-NEO) 300 times, with each result being a 5-dimensional vector representing the Big Five personality traits.

## 2. Methodology & Implementation

We employ multiple methods to visualize and analyze the distribution of these 300 vectors in a high-dimensional space.

### Method 1: Euclidean Distance Distribution

-   **Concept:** This method compresses the 5-dimensional information into a single dimension: the distance from the "average" personality. It calculates the straight-line (Euclidean) distance of each of the 300 vectors from the arithmetic mean of all vectors.
-   **Implementation:** The script `asociety/personality/personality_distribute.py` implements this analysis.
-   **Visualization:** It generates a histogram showing the probability distribution of these distances.
-   **Interpretation:** A distribution tightly clustered around a small distance value indicates high stability.

### Method 2: Mahalanobis Distance Distribution

-   **Concept:** This is a more advanced statistical distance metric. Instead of a simple straight line, it measures the distance of a point from the group's center, taking into account the variance of each trait and the correlation between traits. It essentially measures how many standard deviations away a point is from the mean of the distribution.
-   **Implementation:** The script `asociety/personality/personality_mahalanobis.py` implements this analysis.
-   **Visualization:** It generates a histogram of the Mahalanobis distances.
-   **Interpretation:** This is the gold standard for identifying outliers in a multivariate dataset. A tight distribution indicates that most points are statistically "typical" and conform to the group's characteristics, signifying high stability.

### Method 3: Principal Component Analysis (PCA)

-   **Concept:** PCA is a dimensionality reduction technique that provides a holistic view of the data's structure. Instead of collapsing the information into a single distance, it projects the 5D data onto a 2D plane that captures the maximum possible variance. This allows us to visually inspect the "shape" of the data cloud.
-   **Implementation:** The script `asociety/personality/personality_pca.py` implements this analysis.
-   **Visualization:** It generates a 2D scatter plot where each point represents one of the 300 test results.
-   **Interpretation:** A single, dense, and roughly circular cluster of points on the scatter plot is strong evidence of high stability and convergence.
